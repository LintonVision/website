<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Paul Linton</title>
  
  <meta name="author" content="Paul Linton">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/PaulLinton.jpg">
</head>

<body>
    
    
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Paul Linton</name>
              </p>
              
              <p>
              I am Vision Scientist working on 3D Vision and the author of the book <a href="https://www.palgrave.com/us/book/9783319662923"><em>The Perception and Cognition of Visual Space</em> (Palgrave, 2017)</a>. I am currently a Research Fellow at the <a href="https://www.city.ac.uk/research/centres/applied-vision">Centre for Applied Vision Research</a>, City, University of London where I recently completed my PhD supervised by <a href="https://www.city.ac.uk/about/people/academics/christopher-tyler">Christopher Tyler</a> and <a href="https://www.city.ac.uk/about/people/academics/simon-grant">Simon Grant</a>. Previously, I have been a Research Intern at <a href="https://tech.fb.com/ar-vr/">Facebook Reality Labs</a>, a Law Tutor at <a href="https://www.st-hildas.ox.ac.uk/">St Hilda’s College, Oxford University</a>, and a Teaching Fellow in Philosophy at <a href="https://www.ucl.ac.uk/">University College London</a>.
              </p>
              
              <p>
              My research focuses on 3D visual experience. In my PhD I challenged the longstanding theory that the visual system uses the angular rotation of the eyes to triangulate the size and distance of objects.
              </p>
              <!--
              <p>
              I am co-organising a Royal Society meeting on <a href="https://royalsociety.org/science-events-and-lectures/2021/11/3d-vision/">New Approaches to 3D Vision</a> with representatives from Facebook Reality Labs, Google DeepMind, Google Robotics, and Microsoft Research.
              </p> -->
              
              <p style="text-align:center">
                <a href="mailto:paul@linton.vision">Email</a> &nbsp/&nbsp
                <a href="docs/Paul Linton - Resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="docs/Paul Linton - CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=ehDBNgsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/paul-linton-63206865/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/LintonVision">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/PaulLintonCircle.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PaulLintonCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Royal_Society_Logo.png" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://royalsociety.org/science-events-and-lectures/2021/11/3d-vision/">
                  <papertitle>Royal Society Scientific Meeting New Approaches to 3D Vision (Nov 2021)</papertitle>
                </a>
                </p>
                <p>
                Leading approaches to computer vision (SLAM: simultaneous mapping and localization), animal navigation (cognitive maps), and human vision (optimal cue integration), start from the assumption that the aim of 3D vision is to produce a metric reconstruction of the environment. Recent advances in machine learning, single-cell recording in animals, virtual reality, and visuomotor control, all challenge this assumption. The purpose of this meeting is to bring these different disciplines together to formulate a coherent alternative approach to 3D vision.
                </p>
            </td>
          </tr>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/BookCover.jpg" alt="clean-usnob" height="160">
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.palgrave.com/us/book/9783319662923">
                  <papertitle>Linton, P. <em>The Perception and Cognition of Visual Space</em> (Palgrave Macmillan, 2017)</papertitle>
                </a>
                <br>
                Single author book arguing that 3D vision is a two stage process that involves the extraction of depth from binocular disparity before depth from disparity is integrated with other depth cues. 
                <a href="mailto:paul@linton.vision">Please email for an electronic copy.</a> 
                </p>
          
                <p>Reviewed in <a href="https://journals.sagepub.com/doi/abs/10.1177/0301006618793311"><em>Perception</em></a> by <a href="https://www.uu.nl/staff/CJErkelens/Profile">Casper Erkelens</a> as <a href="docs/ErkelensReview.pdf">"a valuable contribution to the scientific literature on visual perception."</a>
                </p>
                
                <p>Featured on the <a href="https://philosophyofbrains.com/"><em>Brains Blog</em></a>, the leading online forum for cognitive science.
                <br>
                <a href="https://philosophyofbrains.com/2018/06/25/visual-space-and-the-perception-cognition-divide.aspx">Post One</a> /
                <a href="https://philosophyofbrains.com/2018/06/26/perceptual-integration-and-visual-illusions.aspx">Post Two</a> /
                <a href="https://philosophyofbrains.com/2018/06/27/seeing-depth-with-one-eye-and-pictorial-space.aspx">Post Three</a> /
                <a href="https://philosophyofbrains.com/2018/06/28/perceptual-idealism-and-phenomenal-geometry.aspx">Post Four</a> /
                <a href="https://philosophyofbrains.com/2018/06/29/do-we-see-scale.aspx">Post Five</a>
                </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VergenceDistance.jpg" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://link.springer.com/article/10.3758/s13414-020-02006-1">
                  <papertitle>Linton, P. (2020). 'Does Vision Extract Absolute Distance From Vergence?', <em>Attention, Perception, & Psychophysics</em>, 82, 3176–3195 </papertitle>
                </a>
                </p>
                <p>
                  Vergence eye movements are thoughts to play a key role in triangulating the distance of objects, especially at close distances. Accommodation (the focal power of the eye) is also thought to play a role. But these two cues have never been tested divorced from confounding cues such as changes in the retinal image or subjective knowledge about changes in distance. I develop a new paradigm to demonstrate that vergence and accommodation provide no useful metric information once these confounding cues have been controlled for.
                </p>
                <p>
                  Featured on the Psychonomic Society's <a href="https://featuredcontent.psychonomic.org/knocking-a-longstanding-theory-of-distance-perception/">"All Things Cognition"</a> podcast.
                </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VergenceSize.jpg" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.mdpi.com/2411-5150/5/3/33">
                  <papertitle>Linton, P. (2021). 'Does Vergence Affect Perceived Size?', <em>Vision</em>, 5(3), 33 </papertitle>
                </a>
                </p>
                <p>
                  Vergence eye movements are also thought to play an essential role in size perception, in a process known as “size constancy”, which enables us to differentiate a small object up close from a large object far away, even though both cast the same retinal image. I develop a new paradigm to demonstrate that this effect does not exist once cognitive influences (subjective knowledge about changes in distance) have been controlled for.
                </p>
                <p>
                Invited for a special issue on <a href="https://www.mdpi.com/journal/vision/special_issues/size_constancy_perception_action">
                Size Constancy in Perception and Action</a> edited by <a href="https://www.uwo.ca/bmi/goodalelab/">Mel Goodale FRS</a> and <a href="https://psych.ubc.ca/profile/rob-whitwell/">Robert Whitwell</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/V1.jpeg" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
              <p>
              <a href="https://psyarxiv.com/2sv9m/"><papertitle>
              Linton, P. (2021). ‘V1 as an Egocentric Cognitive Map’, <em>Neuroscience of Consciousness</em></a></papertitle>
              </p>
              <p>
                In Linton (2020) and Linton (2021) I find that vergence has either a negligible or no effect on size and distance perception. The question is how to reconcile this finding with the processing of the vergence signal in the primary visual cortex (V1). I argue that we need to distinguish between perceptual and cognitive processing in V1, and draw an analogy with findings on non-visual processing in the mouse V1.
              </p>
              <p>
              Special issue on <a href="https://academic.oup.com/nc/issue/2021/2">Consciousness Science and Its Theories</a>. Contributors include Stanislas Dehaene, Catherine Tallon-Baudry, Biyu Jade He, and Axel Cleeremans.
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Ellipse.jpg" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
                <p><a href="https://www.pnas.org/content/118/10/e2024195118">
                <papertitle>Linton, P., (2021). ‘Conflicting shape percepts explained by perception cognition distinction’, <em>PNAS</em>, 118 (10) e2024195118</papertitle></a>
                </p>
                <p>
                Debate in <a href="https://www.pnas.org/">PNAS</a> with <a href="https://subjectivity.sites.northeastern.edu/">Jorge Morales</a>, Axel Bax, and <a href="https://perception.jhu.edu/chaz/">Chaz Firestone</a> on 3D shape processing in response to <a href="https://www.pnas.org/content/117/26/14873">Morales et al. (2020). ‘Sustained representation of perspectival shape’, PNAS, 117(26), 14873-82</a>. 
              </p>
              <p>
                Provided inspiration for new experiments in <a href="https://www.pnas.org/content/118/28/e2025440118">Morales et al. (2021). ‘Reply to Linton: Perspectival interference up close’, PNAS, 118 (28) e2025440118</a>.
                </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DeepFocus.gif" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
                <p><a href="https://tech.fb.com/introducing-deepfocus/">
                  <papertitle>PhD Internship on the DeepFocus Team at Facebook Reality Labs</papertitle>
                </a><br>
                Manager: <a href="https://sites.google.com/site/marinazannoli/">Marina Zannoli</a>, 
                Collaborator: <a href="https://leixiao-ubc.github.io/">Lei Xiao</a>, 
                Team Lead: <a href="http://alumni.media.mit.edu/~dlanman/">Douglas Lanman</a>
                </p>
                <p>
                Collaborated closely with researchers in deep learning, computer graphics, and optics, as part of a small interdisciplinary team, using artificial intelligence for real-time gaze-contingent defocus blur rendering.
              </p>
              <p>
                Used principles of vision science to inform the development of neural network.
              </p>
              <p>
                Ran user studies to evaluate neural network and make actionable recommendations.
                </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/StereoRendering.jpeg" alt="clean-usnob" width="160">
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1905.10366">
                <papertitle>Linton, P. (2019). ‘Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and Augmented Reality?’, ArXiv, 1905.10366
                </papertitle>
                </a>
                </p>
                <p>
                In contemporary virtual reality displays, the scene is static despite eye movements by the observer. This can cause a problem because the centre of projection of the eye (the nodal point) and the centre of rotation of the eye are off-set relative to one another. 
                </p>
                <p>
                I was the first to propose updating the camera frustum in virtual reality with eye movements to control for these distortions.
                </p>
            </td>
          </tr>
        </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Recent Presentations</heading>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
          </td>
          <td width="75%" valign="middle">

            <a href="https://www.osa.org/en-us/meetings/webinar/2021/06_june/depth_perception_in_ar_vr_optics_graphics_and_cont/">
              <papertitle>Optical Society (OSA), Technical Group on Display Technology, "Depth Perception in AR/VR: Optics, Graphics and Content Virtual Panel Discussion" (June 2021)</papertitle><br><br>
            </a>

            Virtual Environments and Computer Graphics Group, UCL (June 2021)
            <br><br>

            <a href="https://jov.arvojournals.org/article.aspx?articleid=2770977&resultClick=1">
              <papertitle>Vision Sciences Society (May 2020) (Elsevier / Vision Research Travel Award)</papertitle><br><br>
            </a>

            <a href="https://britishmachinevisionassociation.github.io/meetings/20-01-29-3D%20worlds%20from%202D%20images%20in%20humans%20and%20machines.html">
              <papertitle>British Machine Vision Association, "Symposium 3D Worlds From 2D Images" (Jan 2020)</papertitle><br><br>
            </a>
                    <iframe width="624" height="351" src="https://www.youtube.com/embed/6P3EYCEn52A" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <br><br>

          </td>
        </tr>
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the html template.
              </p>
            </td>
          </tr>
        </tbody></table>
        
      </td>
    </tr>
  </table>
</body>

</html>
